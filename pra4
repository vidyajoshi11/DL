 # 1. Load and preprocess the dataset
 import numpy as np
 import matplotlib.pyplot as plt
 from sklearn.preprocessing import MinMaxScaler
 
 # Generate synthetic time series data (sine wave)
 data = np.sin(np.arange(0, 100, 0.1))
 data = data.reshape(-1, 1)
 
 # Scale the data
 scaler = MinMaxScaler(feature_range=(0, 1))
 data = scaler.fit_transform(data)
 
 # Create sequences for RNN input
 def create_sequences(data, sequence_length):
 X, y = [], []
 for i in range(len(data) - sequence_length):
 X.append(data[i:(i + sequence_length), 0])
 y.append(data[i + sequence_length, 0])
 return np.array(X), np.array(y)
 sequence_length = 10
 X, y = create_sequences(data, sequence_length)
 
 # Reshape X for RNN input (samples, time steps, features)
 X = X.reshape(X.shape[0], X.shape[1], 1)

  # 2. Split the data
 from sklearn.model_selection import train_test_split
 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

 # 3. Build the RNN model
 import tensorflow as tf
 from tensorflow.keras.models import Sequential
 from tensorflow.keras.layers import SimpleRNN, Dense
 model = Sequential([
 SimpleRNN(50, activation='relu', input_shape=(sequence_length, 1)),
 Dense(1)
 ])

  # 4. Compile the model
 model.compile(optimizer='adam', loss='mse')

 # 5. Train the model
 epochs = 10
 history = model.fit(X_train, y_train,
                    epochs=epochs,
                    verbose=1)


   # 6. Evaluate the model
 loss = model.evaluate(X_test, y_test, verbose=0)
 print('Test loss:', loss)


 # 7. Make predictions
 predictions = model.predict(X_test) predictions = scaler.inverse_transform(predictions)
 y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))
 
 # Plot the predictions vs actual values
 plt.figure(figsize=(12, 6))
 plt.plot(y_test_original, label='Actual')
 plt.plot(predictions, label='Predictions')
 plt.title('Time Series Prediction')
 plt.xlabel('Time Step')
 plt.ylabel('Value')
 plt.legend()
 plt.show()
